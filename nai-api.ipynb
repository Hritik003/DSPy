{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "url = \"https://ai.nutanix.com/api/v1/chat/completions\" \n",
    "\n",
    "nai_api_key = os.getenv(\"NAI_API_KEY\")\n",
    "\n",
    "# print(nai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hritik.raj/DSPy/nai-api/lib/python3.12/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ai.nutanix.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 2.651865005493164 seconds\n",
      "Response from the API:\n",
      "{'id': 'd2d3943d-177f-4460-8286-6eaa80970c4f', 'object': 'chat.completion', 'created': 1729682716, 'model': 'vllm-llama-3-1', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"To determine how many eggs are left after 5 are taken out, we'll follow the steps below:\\n\\n1. We start with 15 eggs.\\n2. Next, 5 eggs are taken out. To find out how many eggs are left, we subtract 5 from 15.\\n3. 15 - 5 = 10\\n\\nSo, there are 10 eggs left.\"}, 'finish_reason': 'stop', 'content_filter_results': {'hate': {'filtered': False}, 'self_harm': {'filtered': False}, 'sexual': {'filtered': False}, 'violence': {'filtered': False}, 'jailbreak': {'filtered': False, 'detected': False}, 'profanity': {'filtered': False, 'detected': False}}}], 'usage': {'prompt_tokens': 62, 'completion_tokens': 79, 'total_tokens': 141, 'prompt_tokens_details': None, 'completion_tokens_details': None}, 'system_fingerprint': ''}\n"
     ]
    }
   ],
   "source": [
    "question = \"produce the answer. We start with 15 eggs and if 5 are taken out how many are left? think step by step \"\n",
    "\n",
    "start_time = time.time()\n",
    "response = requests.post(url, \n",
    "                         headers={\n",
    "                                \"Authorization\": f\"Bearer {nai_api_key}\", \n",
    "                                \"accept\": \"application/json\",\n",
    "                                \"Content-Type\": \"application/json\"\n",
    "                            }, \n",
    "                         json={\n",
    "                                \"model\": \"vllm-llama-3-1\",\n",
    "                                \"messages\": [\n",
    "                                    {\n",
    "                                        \"role\": \"user\",\n",
    "                                        \"content\": f\"{question}\"\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"max_tokens\": 256,\n",
    "                                \"stream\": False\n",
    "                            }, \n",
    "                         verify=False)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time} seconds\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Response from the API:\")\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(f\"Failed with status code: {response.status_code}\")\n",
    "    print(\"Error message:\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the number of eggs left, we start with 15 and subtract the 5 eggs that were taken out.\n",
      "\n",
      "15 (initial eggs) - 5 (eggs taken out) = 10\n",
      "\n",
      "There are 10 eggs left.\n"
     ]
    }
   ],
   "source": [
    "print(response.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 : Refinning the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My primary refinning techniques would be:\n",
    "\n",
    "- Clarity\n",
    "- Context\n",
    "- Conciseness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Prompt?\n",
    "\n",
    "Deciding on a base prompt is an important step, and it depends on the task we are actually trying to accomplish. Lets start with a data analysis example, assuming the client wants to generate insights from a sales report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "with open('prompts/prompt_01.txt', 'r') as file:\n",
    "    base_prompt = file.read()\n",
    "\n",
    "print(base_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Define Rules for clarity, context and conciseness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_rules={\n",
    "    \"clarity\":{\n",
    "        \"summarization\":\"Summarize the following text cleary and coincisely.\",\n",
    "        \"generation\":\"Generate a response that clearly addresses the request.\",\n",
    "        \"QA\":\"Answer the Question based on the provided information.\"\n",
    "    },\n",
    "    \"context\":{\n",
    "        \"summarization\":\"The text is about [subject/Topic]\",\n",
    "        \"generation\":\"Make sure to include context about [subject/topic]\",\n",
    "        \"QA\":\"Use the context of [subject/topic] to provide a relevant answer.\"\n",
    "    },\n",
    "    \"conciseness\":{\n",
    "        \"summarization\": \"Limit the summary to under [word_count] words.\",\n",
    "        \"generation\": \"Keep the response concise and focused on the main idea.\",\n",
    "        \"QA\": \"Provide a direct and concise answer.\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have defined the rules, maybe now we will have to classify the `prompt` among these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_clarity(prompt):\n",
    "    \"\"\"Classify the prompt type based on the keywords or structure\"\"\"\n",
    "    if \"summarize\" in prompt.lower():\n",
    "        return \"summarization\"\n",
    "    elif \"generate\" in prompt.lower():\n",
    "        return \"generation\"\n",
    "    elif \"answer\" in prompt.lower():\n",
    "        return \"QA\"\n",
    "    else:\n",
    "        return \"generic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clarity(prompt_type, prompt):\n",
    "    \"\"\"Apply the clarity rules based on the prompt type\"\"\"\n",
    "    if prompt_type in generic_rules[\"clarity\"]:\n",
    "        return generic_rules[\"clarity\"][prompt_type]+\" \"+prompt\n",
    "    return prompt\n",
    "\n",
    "def apply_context(prompt_type, prompt,subject=\"the provided information\"):\n",
    "    \"\"\"Apply the context rule based on the prompt type\"\"\"\n",
    "    if prompt_type in generic_rules[\"context\"]:\n",
    "        return generic_rules[\"context\"][prompt_type].replace(\"[subject/topic]\", subject)+\" \"+prompt\n",
    "    return prompt\n",
    "\n",
    "def apply_conciseness(prompt_type, prompt, word_count=100):\n",
    "    \"\"\"Apply consciseness rule based on prompt type\"\"\"\n",
    "    if prompt_type in generic_rules[\"conciseness\"]:\n",
    "        return generic_rules[\"conciseness\"][prompt_type].replace(\"[word_count]\", str(word_count)) + \" \" + prompt\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nai-api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
