{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "url = \"https://ai.nutanix.com/api/v1/chat/completions\" \n",
    "\n",
    "nai_api_key = os.getenv(\"NAI_API_KEY\")\n",
    "\n",
    "# print(nai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hritik.raj/DSPy/nai-api/lib/python3.12/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ai.nutanix.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 2.651865005493164 seconds\n",
      "Response from the API:\n",
      "{'id': 'd2d3943d-177f-4460-8286-6eaa80970c4f', 'object': 'chat.completion', 'created': 1729682716, 'model': 'vllm-llama-3-1', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"To determine how many eggs are left after 5 are taken out, we'll follow the steps below:\\n\\n1. We start with 15 eggs.\\n2. Next, 5 eggs are taken out. To find out how many eggs are left, we subtract 5 from 15.\\n3. 15 - 5 = 10\\n\\nSo, there are 10 eggs left.\"}, 'finish_reason': 'stop', 'content_filter_results': {'hate': {'filtered': False}, 'self_harm': {'filtered': False}, 'sexual': {'filtered': False}, 'violence': {'filtered': False}, 'jailbreak': {'filtered': False, 'detected': False}, 'profanity': {'filtered': False, 'detected': False}}}], 'usage': {'prompt_tokens': 62, 'completion_tokens': 79, 'total_tokens': 141, 'prompt_tokens_details': None, 'completion_tokens_details': None}, 'system_fingerprint': ''}\n"
     ]
    }
   ],
   "source": [
    "question = \"produce the answer. We start with 15 eggs and if 5 are taken out how many are left? think step by step \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the number of eggs left, we start with 15 and subtract the 5 eggs that were taken out.\n",
      "\n",
      "15 (initial eggs) - 5 (eggs taken out) = 10\n",
      "\n",
      "There are 10 eggs left.\n"
     ]
    }
   ],
   "source": [
    "print(response.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_llm(prompt):\n",
    "    response = requests.post(url, \n",
    "                         headers={\n",
    "                                \"Authorization\": f\"Bearer {nai_api_key}\", \n",
    "                                \"accept\": \"application/json\",\n",
    "                                \"Content-Type\": \"application/json\"\n",
    "                            }, \n",
    "                         json={\n",
    "                                \"model\": \"vllm-llama-3-1\",\n",
    "                                \"messages\": [\n",
    "                                    {\n",
    "                                        \"role\": \"user\",\n",
    "                                        \"content\": f\"{prompt}\"\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"max_tokens\": 256,\n",
    "                                \"stream\": False\n",
    "                            }, \n",
    "                         verify=False)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 : Refinning the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My primary refinning techniques would be:\n",
    "\n",
    "- Clarity\n",
    "- Context\n",
    "- Conciseness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Prompt?\n",
    "\n",
    "Deciding on a base prompt is an important step, and it depends on the task we are actually trying to accomplish. Lets start with a data analysis example, assuming the client wants to generate insights from a sales report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyze the following sales report and provide a summary of the top 3 performing products along with their respective revenue\n"
     ]
    }
   ],
   "source": [
    "with open('prompts/prompt_01.prompt', 'r') as file:\n",
    "    base_prompt = file.read()\n",
    "\n",
    "print(base_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Define Rules for clarity, context and conciseness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_rules={\n",
    "    \"clarity\":{\n",
    "        \"summarization\":\"Summarize the following text cleary and coincisely.\",\n",
    "        \"generation\":\"Generate a response that clearly addresses the request.\",\n",
    "        \"QA\":\"Answer the Question based on the provided information.\"\n",
    "    },\n",
    "    \"context\":{\n",
    "        \"summarization\":\"The text is about [subject/Topic]\",\n",
    "        \"generation\":\"Make sure to include context about [subject/topic]\",\n",
    "        \"QA\":\"Use the context of [subject/topic] to provide a relevant answer.\"\n",
    "    },\n",
    "    \"conciseness\":{\n",
    "        \"summarization\": \"Limit the summary to under [word_count] words.\",\n",
    "        \"generation\": \"Keep the response concise and focused on the main idea.\",\n",
    "        \"QA\": \"Provide a direct and concise answer.\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have defined the rules, maybe now we will have to classify the `prompt` among these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_prompt(prompt):\n",
    "    \"\"\"Classify the prompt type based on the keywords or structure\"\"\"\n",
    "    if \"summarize\" in prompt.lower():\n",
    "        return \"summarization\"\n",
    "    elif \"generate\" in prompt.lower():\n",
    "        return \"generation\"\n",
    "    elif \"answer\" in prompt.lower():\n",
    "        return \"QA\"\n",
    "    else:\n",
    "        return \"generic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def apply_clarity(prompt, subject=None):\n",
    "    \"\"\"Add clarity to any prompt, dynamically incorporating subject if provided\"\"\"\n",
    "    clarity_template = \"Provide a clear and concise response for the following task.\"\n",
    "    if subject:\n",
    "        clarity_template = f\"Provide a clear and concise response related to {subject}.\"\n",
    "    refined = clarity_template + \" \" + prompt\n",
    "    return refined\n",
    "\n",
    "def apply_context(prompt, context_info=None):\n",
    "    \"\"\"Dynamically apply context if available or use a general placeholder\"\"\"\n",
    "    if context_info:\n",
    "        context_template = f\"Use the context of {context_info} to perform the task.\"\n",
    "    else:\n",
    "        context_template = \"Use the given context to perform the task.\"\n",
    "    return context_template + \" \" + prompt\n",
    "\n",
    "def apply_conciseness(prompt, word_limit=100):\n",
    "    \"\"\"Ensure conciseness by dynamically applying word limits\"\"\"\n",
    "    conciseness_template = f\"Keep the response concise and under {word_limit} words.\"\n",
    "    return conciseness_template + \" \" + prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined Prompt (Generic): Keep the response concise and under 100 words. Use the given context to perform the task. Provide a clear and concise response for the following task. Analyze the following data and provide insights.\n",
      "\n",
      "Refined Prompt (With Subject): Keep the response concise and under 150 words. Use the context of customer reviews to perform the task. Provide a clear and concise response related to customer feedback. Analyze the following data and provide insights.\n"
     ]
    }
   ],
   "source": [
    "def automate_refinement_generic(base_prompt, subject=None, context_info=None, word_limit=100):\n",
    "    \"\"\"Automate prompt refinement for any prompt type with dynamic subject, context, and conciseness rules\"\"\"\n",
    "    \n",
    "    refined_prompt = apply_clarity(base_prompt, subject)\n",
    "    refined_prompt = apply_context(refined_prompt, context_info)\n",
    "    refined_prompt = apply_conciseness(refined_prompt, word_limit)\n",
    "    \n",
    "    return refined_prompt\n",
    "\n",
    "refined_prompt_generic = automate_refinement_generic(base_prompt)\n",
    "\n",
    "refined_prompt_with_subject = automate_refinement_generic(base_prompt, subject=\"customer feedback\", context_info=\"customer reviews\", word_limit=150)\n",
    "\n",
    "print(\"Refined Prompt (Generic):\", refined_prompt_generic)\n",
    "print(\"\\nRefined Prompt (With Subject):\", refined_prompt_with_subject)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hritik.raj/DSPy/nai-api/lib/python3.12/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ai.nutanix.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, I don't see any data provided in your question. If you provide the customer review data, I'd be happy to analyze and provide insights. Please paste the data, and I'll get started.\n",
      "\n",
      "If you have any example customer review ratings or feedback, you can provide it, and I'll give you a concise analysis under 150 words.\n",
      "\n",
      "Additionally, what type of insights would you like to gain from the analysis? Are you looking for customer pain points, common issues, or areas of improvement?\n"
     ]
    }
   ],
   "source": [
    "print(get_response_llm(refined_prompt_with_subject).json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DynamicPrompt' from 'dspy' (/Users/hritik.raj/DSPy/nai-api/lib/python3.12/site-packages/dspy/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdspy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DynamicPrompt\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DynamicPrompt' from 'dspy' (/Users/hritik.raj/DSPy/nai-api/lib/python3.12/site-packages/dspy/__init__.py)"
     ]
    }
   ],
   "source": [
    "from dspy import Dyanamic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nai-api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
